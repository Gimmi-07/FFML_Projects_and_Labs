{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gimmi-07/FFML_Projects_and_Labs/blob/main/CAPSTONE_01_Fundamentals_of_DEEP_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fundamentals of Deep Learning**\n",
        "\n",
        "We will start with the Fundamentals of the deep learning. Firstly we will understand \"What is deep learning?\".\n",
        "\n",
        "Deep learning is a type of machine learning that uses artificial neural networks to learn from data in a way that mimics how the human brain works. It is called \"deep\" because the neural networks have multiple layers that enable the model to learn increasingly complex features of the data.\n",
        "\n",
        "To understand this concept we will start with an basic example of Image classification. Let's say we want to train a deep learning model to classify images of animals. We would provide the model with a large dataset of labeled animal images, like cats, dogs, and horses. It will covered in several methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "PaUC-B7Y-_1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Importing the Necessary Libraries\n",
        "\n",
        "Now we will import the required liabraries."
      ],
      "metadata": {
        "id": "NnxOdJi2_vem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkDVLb7t--pF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're using the CIFAR-10 dataset in which it contains 60,000 (32x32) color images in 10 classes, such as dogs, cats, airplanes and many more."
      ],
      "metadata": {
        "id": "Wedn38yR_6NX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Defining the Deep Learning Model\n",
        "\n",
        "Now, we'll define a simple neural network with three fully connected layers. The forward() method defines the forward pass of the network.\n"
      ],
      "metadata": {
        "id": "5F_Kguc2A2Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oUEGSsAlMh7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading and Preprocessing the Data\n",
        "\n",
        "We use the torchvision.datasets.MNIST module to download and load the MNIST dataset. We apply a transformation to normalize the input images.\n",
        "\n",
        "Here we are using the nn.CrossEntropyLoss as the loss function and optim.SGD as the optimizer for our MNIST Dataset."
      ],
      "metadata": {
        "id": "ay5pJUR6ATFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCavlgITAn4U",
        "outputId": "d3ae3a4a-72d8-4bba-fbd7-c29bd0663342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35626877.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1103058.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9745331.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2887756.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training the Model\n",
        "\n",
        "This step is an important aspect Deep learning. Here we are training the model for 10 epochs, using a batch size of the 64. In each iteration, we forward the input through the network, computing the loss, backpropagating the gradients, and updating the model parameters."
      ],
      "metadata": {
        "id": "cH94aqtcB2XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSFXmSfQCjR5",
        "outputId": "4e5e1499-0e73-4a90-8466-1942e7db4f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 2.216\n",
            "[1,   200] loss: 1.829\n",
            "[1,   300] loss: 1.186\n",
            "[1,   400] loss: 0.754\n",
            "[1,   500] loss: 0.585\n",
            "[1,   600] loss: 0.474\n",
            "[1,   700] loss: 0.442\n",
            "[1,   800] loss: 0.398\n",
            "[1,   900] loss: 0.384\n",
            "[2,   100] loss: 0.353\n",
            "[2,   200] loss: 0.336\n",
            "[2,   300] loss: 0.334\n",
            "[2,   400] loss: 0.318\n",
            "[2,   500] loss: 0.322\n",
            "[2,   600] loss: 0.299\n",
            "[2,   700] loss: 0.307\n",
            "[2,   800] loss: 0.289\n",
            "[2,   900] loss: 0.294\n",
            "[3,   100] loss: 0.289\n",
            "[3,   200] loss: 0.275\n",
            "[3,   300] loss: 0.270\n",
            "[3,   400] loss: 0.254\n",
            "[3,   500] loss: 0.252\n",
            "[3,   600] loss: 0.258\n",
            "[3,   700] loss: 0.244\n",
            "[3,   800] loss: 0.253\n",
            "[3,   900] loss: 0.240\n",
            "[4,   100] loss: 0.236\n",
            "[4,   200] loss: 0.225\n",
            "[4,   300] loss: 0.229\n",
            "[4,   400] loss: 0.223\n",
            "[4,   500] loss: 0.216\n",
            "[4,   600] loss: 0.208\n",
            "[4,   700] loss: 0.203\n",
            "[4,   800] loss: 0.224\n",
            "[4,   900] loss: 0.212\n",
            "[5,   100] loss: 0.205\n",
            "[5,   200] loss: 0.200\n",
            "[5,   300] loss: 0.191\n",
            "[5,   400] loss: 0.192\n",
            "[5,   500] loss: 0.207\n",
            "[5,   600] loss: 0.198\n",
            "[5,   700] loss: 0.176\n",
            "[5,   800] loss: 0.172\n",
            "[5,   900] loss: 0.173\n",
            "[6,   100] loss: 0.173\n",
            "[6,   200] loss: 0.157\n",
            "[6,   300] loss: 0.179\n",
            "[6,   400] loss: 0.159\n",
            "[6,   500] loss: 0.178\n",
            "[6,   600] loss: 0.174\n",
            "[6,   700] loss: 0.159\n",
            "[6,   800] loss: 0.166\n",
            "[6,   900] loss: 0.161\n",
            "[7,   100] loss: 0.146\n",
            "[7,   200] loss: 0.148\n",
            "[7,   300] loss: 0.153\n",
            "[7,   400] loss: 0.141\n",
            "[7,   500] loss: 0.157\n",
            "[7,   600] loss: 0.149\n",
            "[7,   700] loss: 0.152\n",
            "[7,   800] loss: 0.146\n",
            "[7,   900] loss: 0.150\n",
            "[8,   100] loss: 0.141\n",
            "[8,   200] loss: 0.133\n",
            "[8,   300] loss: 0.133\n",
            "[8,   400] loss: 0.142\n",
            "[8,   500] loss: 0.138\n",
            "[8,   600] loss: 0.130\n",
            "[8,   700] loss: 0.122\n",
            "[8,   800] loss: 0.131\n",
            "[8,   900] loss: 0.138\n",
            "[9,   100] loss: 0.125\n",
            "[9,   200] loss: 0.118\n",
            "[9,   300] loss: 0.133\n",
            "[9,   400] loss: 0.121\n",
            "[9,   500] loss: 0.118\n",
            "[9,   600] loss: 0.121\n",
            "[9,   700] loss: 0.118\n",
            "[9,   800] loss: 0.127\n",
            "[9,   900] loss: 0.115\n",
            "[10,   100] loss: 0.105\n",
            "[10,   200] loss: 0.124\n",
            "[10,   300] loss: 0.112\n",
            "[10,   400] loss: 0.120\n",
            "[10,   500] loss: 0.105\n",
            "[10,   600] loss: 0.112\n",
            "[10,   700] loss: 0.106\n",
            "[10,   800] loss: 0.114\n",
            "[10,   900] loss: 0.100\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluating the Model\n",
        "\n",
        "After training the dataset, we can evaluate the model's performance based on the test set. We are computing the overall accuracy of the network on the 10,000 test images.\n",
        "\n"
      ],
      "metadata": {
        "id": "yr0W10HiCt-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT9vsVlQDFAE",
        "outputId": "bb9e8497-44e9-40e0-ffc0-db3c3b0d78bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 96 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSION:\n",
        "\n",
        "This simple example provides a solid foundation for understanding the basic structure and workflow of training a deep learning model using PyTorch. It covers the essential components, such as defining the network architecture, loading and preprocessing data, setting up the training process, and evaluating the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "oplb5oWeDmq5"
      }
    }
  ]
}