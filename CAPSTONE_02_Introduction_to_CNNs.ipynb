{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gimmi-07/FFML_Projects_and_Labs/blob/main/CAPSTONE_02_Introduction_to_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convulation Neural Network**\n",
        "\n",
        "Here we are going to explore the concept of the CNN in Deep learning.\n",
        "\n",
        "### What is Convulation Neural Network (CNNs) ?\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a type of deep learning model used mainly for image recognition tasks. It works by automatically learning to recognize patterns in images through layers called convolutional layers. These layers use filters to scan over the input image and detect features like edges, textures, and shapes.\n",
        "\n",
        "As the data passes through multiple layers, the CNN learns more complex patterns. CNNs also include pooling layers to reduce the size of the data and make the model more efficient. Finally, fully connected layers at the end of the network use the learned features to classify the images into categories. CNNs are powerful because they can automatically and adaptively learn spatial hierarchies of features from input images.\n",
        "\n",
        "We will consider the example of program that demonstrates the fundamentals of convolutional neural networks by using PyTorch library."
      ],
      "metadata": {
        "id": "NQRIJdnbGniQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing the Libraries\n",
        "\n",
        "First, we'll import the necessary PyTorch and other libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "DAVbWfKJQg-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4vDZ7uoF2C3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loading and Preprocessing Data\n",
        "\n",
        "Next, we are going to load and preprocess the CIFAR-10 dataset with data augmentation to improve model generalization."
      ],
      "metadata": {
        "id": "zPG9fJrfREXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to normalize the data and apply data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download and load training data\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Download and load test data\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wBvomL5RoLD",
        "outputId": "4e768810-c19c-4430-84a4-58024db2f5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Defining the CNN Model\n",
        "\n",
        "We define a deeper and more complex CNN model, and include printing the shape to ensure it matches."
      ],
      "metadata": {
        "id": "AYqrYbmZRnK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.25))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(p=0.25))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(p=0.25))\n",
        "        self.fc1 = nn.Linear(128 * 2 * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = AdvancedCNN()\n"
      ],
      "metadata": {
        "id": "jMYYBL_TSDkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Defining the Loss Function and Optimizer\n",
        "We specify the loss function and use the Adam optimizer."
      ],
      "metadata": {
        "id": "d7qZIWdJSD0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "g003YxGKSTDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training the Model\n",
        "\n",
        "We training the model on the training data."
      ],
      "metadata": {
        "id": "5RN1POPKTqHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYhMMK4tT55m",
        "outputId": "377cf510-5d42-4a65-baac-7d167329f474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.994\n",
            "Epoch 1, Batch 200, Loss: 1.723\n",
            "Epoch 1, Batch 300, Loss: 1.627\n",
            "Epoch 2, Batch 100, Loss: 1.502\n",
            "Epoch 2, Batch 200, Loss: 1.459\n",
            "Epoch 2, Batch 300, Loss: 1.429\n",
            "Epoch 3, Batch 100, Loss: 1.373\n",
            "Epoch 3, Batch 200, Loss: 1.351\n",
            "Epoch 3, Batch 300, Loss: 1.320\n",
            "Epoch 4, Batch 100, Loss: 1.301\n",
            "Epoch 4, Batch 200, Loss: 1.277\n",
            "Epoch 4, Batch 300, Loss: 1.294\n",
            "Epoch 5, Batch 100, Loss: 1.245\n",
            "Epoch 5, Batch 200, Loss: 1.213\n",
            "Epoch 5, Batch 300, Loss: 1.238\n",
            "Epoch 6, Batch 100, Loss: 1.199\n",
            "Epoch 6, Batch 200, Loss: 1.177\n",
            "Epoch 6, Batch 300, Loss: 1.172\n",
            "Epoch 7, Batch 100, Loss: 1.152\n",
            "Epoch 7, Batch 200, Loss: 1.156\n",
            "Epoch 7, Batch 300, Loss: 1.150\n",
            "Epoch 8, Batch 100, Loss: 1.125\n",
            "Epoch 8, Batch 200, Loss: 1.100\n",
            "Epoch 8, Batch 300, Loss: 1.105\n",
            "Epoch 9, Batch 100, Loss: 1.073\n",
            "Epoch 9, Batch 200, Loss: 1.085\n",
            "Epoch 9, Batch 300, Loss: 1.076\n",
            "Epoch 10, Batch 100, Loss: 1.052\n",
            "Epoch 10, Batch 200, Loss: 1.057\n",
            "Epoch 10, Batch 300, Loss: 1.082\n",
            "Epoch 11, Batch 100, Loss: 1.051\n",
            "Epoch 11, Batch 200, Loss: 1.019\n",
            "Epoch 11, Batch 300, Loss: 1.034\n",
            "Epoch 12, Batch 100, Loss: 1.009\n",
            "Epoch 12, Batch 200, Loss: 1.031\n",
            "Epoch 12, Batch 300, Loss: 1.018\n",
            "Epoch 13, Batch 100, Loss: 1.024\n",
            "Epoch 13, Batch 200, Loss: 0.998\n",
            "Epoch 13, Batch 300, Loss: 1.003\n",
            "Epoch 14, Batch 100, Loss: 0.996\n",
            "Epoch 14, Batch 200, Loss: 0.996\n",
            "Epoch 14, Batch 300, Loss: 0.990\n",
            "Epoch 15, Batch 100, Loss: 0.971\n",
            "Epoch 15, Batch 200, Loss: 0.972\n",
            "Epoch 15, Batch 300, Loss: 0.976\n",
            "Epoch 16, Batch 100, Loss: 0.974\n",
            "Epoch 16, Batch 200, Loss: 0.962\n",
            "Epoch 16, Batch 300, Loss: 0.967\n",
            "Epoch 17, Batch 100, Loss: 0.939\n",
            "Epoch 17, Batch 200, Loss: 0.960\n",
            "Epoch 17, Batch 300, Loss: 0.956\n",
            "Epoch 18, Batch 100, Loss: 0.951\n",
            "Epoch 18, Batch 200, Loss: 0.949\n",
            "Epoch 18, Batch 300, Loss: 0.949\n",
            "Epoch 19, Batch 100, Loss: 0.925\n",
            "Epoch 19, Batch 200, Loss: 0.934\n",
            "Epoch 19, Batch 300, Loss: 0.939\n",
            "Epoch 20, Batch 100, Loss: 0.919\n",
            "Epoch 20, Batch 200, Loss: 0.929\n",
            "Epoch 20, Batch 300, Loss: 0.893\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluating the Model\n",
        "We evaluate the model on the test data."
      ],
      "metadata": {
        "id": "7EMDkKOcXos_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd7L7dLXXtm_",
        "outputId": "628a8cc1-6cd5-4e06-b87c-df5893d8e25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 67.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSION:\n",
        "\n",
        "This advanced CNN model includes several features that improve its performance and accuracy. Firstly, it uses data augmentation techniques, such as random horizontal flipping and random cropping, which help the model generalize better to new data. The model itself is deeper, with more convolutional layers that increase its complexity and allow it to learn more intricate patterns from the data. Batch normalization is used to stabilize the learning process, making the training more efficient and effective.\n",
        "\n",
        "Additionally, dropout layers are included to regularize the model and prevent overfitting by randomly deactivating certain neurons during training. Finally, the Adam optimizer is employed, which is an advanced optimization algorithm known for its efficiency and good performance in training deep learning models. These combined features make this CNN model more robust and capable of achieving higher accuracy compared to simpler models."
      ],
      "metadata": {
        "id": "uFuFxM-EYS6A"
      }
    }
  ]
}